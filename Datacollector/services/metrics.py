

def macro_accuracy(accuracy_dict):
    labs = list(accuracy_dict.keys())
    macro_acc = {}
    for lab in labs:
        lab_accuracy_dict = accuracy_dict[lab]
        all_accuracies = list(lab_accuracy_dict.values())
        avg_acc = sum(all_accuracies) / len(all_accuracies)
        macro_acc[lab] = avg_acc
    return macro_acc


def macro_overall_accuracy(lab_accuracy_dict):
    accuracies = list(lab_accuracy_dict.values())
    avg_accuracy = sum(accuracies) / len(accuracies)
    return avg_accuracy


def micro_overall_accuracy(accuracy_dict, test_numbers):
    macro_acc = macro_accuracy(accuracy_dict)
    labs = list(macro_acc.keys())
    overall_acc = 0
    cumulative_accuracy = 0
    total_nos = 0
    for lab in labs:
        cumulative_accuracy += macro_acc[lab] * test_numbers[lab]
        total_nos += test_numbers[lab]
    overall_acc = cumulative_accuracy / total_nos
    return overall_acc

def calculate_metrics(original_grades, model_grades):
    """
    Calculates metrics like accuracy by comparing TA grades and grades assigned by model
    """
    correct = 0
    one_off_correct = 0

    higher_by_model = 0  # Model assigns a better grade
    lower_by_model = 0  # Model assigns a worse grade

    tp = {}
    fp = {}
    tn = {}
    fn = {}

    for student_id in original_grades.keys():
        tp[original_grades[student_id].strip()] = 0
        fp[original_grades[student_id].strip()] = 0
        tn[original_grades[student_id].strip()] = 0
        fn[original_grades[student_id].strip()] = 0

    for student_id in model_grades.keys():
        tp[model_grades[student_id].strip()] = 0
        fp[model_grades[student_id].strip()] = 0
        tn[model_grades[student_id].strip()] = 0
        fn[model_grades[student_id].strip()] = 0

    total = 0
    for student_id in model_grades.keys():
        if student_id in original_grades.keys():
            model_grade = ord(model_grades[student_id].strip()) - ord("A")
            original_grades[student_id] = original_grades[student_id].strip()

            if len(original_grades[student_id]) != 1:
                continue
            original_grade = ord(original_grades[student_id]) - ord("A")

            total += 1
            if model_grade == original_grade:
                correct += 1
                tp[model_grades[student_id].strip()] += 1
                for grade in tp.keys():
                    if grade != model_grades[student_id].strip():
                        tn[grade] += 1
            else:
                fp[model_grades[student_id].strip()] += 1
                fn[original_grades[student_id].strip()] += 1
                for grade in tp.keys():
                    if (grade != model_grades[student_id].strip()) and (
                        grade != original_grades[student_id].strip()
                    ):
                        tn[grade] += 1

            if abs(model_grade - original_grade) <= 1:
                one_off_correct += 1

            if model_grade > original_grade:
                lower_by_model += 1

            if model_grade < original_grade:
                higher_by_model += 1

    if total == 0:
        print("THIS LAB Output format generated by model is invalid")
        return

    # print(f"Total number of submissions : {total}")
    precisions = {}
    recalls = {}
    f1_scores = []
    microacc = []
    total_tp = 0
    total_tn = 0
    total_fp = 0
    total_fn = 0
    # print(tp, fp, tn, fn)
    for grade in tp.keys():
        total_tp += tp[grade]
        total_fp += fp[grade]
        total_tn += tn[grade]
        total_fn += fn[grade]

        # print(grade, tp[grade], fp[grade], tn[grade], fn[grade])

        if (tp[grade] + fp[grade] > 0) and (tp[grade] + fn[grade] > 0):
            precisions[grade] = tp[grade] / (tp[grade] + fp[grade])
            recalls[grade] = tp[grade] / (tp[grade] + fn[grade])
            microacc.append(
                (tp[grade] + tn[grade])
                / (tp[grade] + fp[grade] + tn[grade] + fn[grade])
            )
            if precisions[grade] + recalls[grade] == 0:
                f1_scores.append(0)

            else:
                f1_scores.append(
                    2
                    * (precisions[grade] * recalls[grade])
                    / (precisions[grade] + recalls[grade])
                )

        if (tp[grade] + fp[grade] == 0) and fn[grade] > 0:
            precisions[grade] = 0
            recalls[grade] = 0
            f1_scores.append(0)
            microacc.append(0)

        if (tp[grade] == 0) and (fp[grade] == 0) and (fn[grade] == 0):
            precisions[grade] = 1
            recalls[grade] = 1
            f1_scores.append(1)
            microacc.append(1)
    count = 0
    avg_precision = 0
    avg_recall = 0
    for grade in precisions.keys():
        count += 1
        avg_precision += precisions[grade]
        avg_recall += recalls[grade]
    if count > 0:
        avg_precision = avg_precision / count
        avg_recall = avg_recall / count
        # print(f"Avg precision : {avg_precision}")
        # print(f"Avg recall : {avg_recall}")
    micro_tp = total_tp
    micro_tpfp = total_tp + total_fp
    micro_tpfn = total_tp + total_fn
    micro_tptn = total_tp + total_tn
    micro_all = total_tp + total_tn + total_fn + total_fp
    macro_accuracy = 0
    total_grades = len(tp)  # Total number of grades ('A', 'B', 'C', 'D')
    acc = []
    for grade in tp.keys():
        grade_accuracy = (tp[grade] + tn[grade]) / (
            tp[grade] + fp[grade] + tn[grade] + fn[grade]
        )
        macro_accuracy += grade_accuracy
        acc.append(grade_accuracy)

    # Macro Accuracy is the average accuracy across all grades
    macro_accuracy /= total_grades

    json_obj = {
        "macroacc": acc,  # Micro Precision
        "macrof1": f1_scores,  # Micro Precision
        "micro_tpfp": micro_tpfp,  # Micro Precision
        "micro_tpfn": micro_tpfn,  # Micro Recall
        "micro_tptn": micro_tptn,  # Micro F1 Score
        "micro_tp": micro_tp,  # Total True Positives (Micro)
        "micro_total": micro_all,
        "count": total,
        "accuracy": correct / total,
        "macro_accuracy": macro_accuracy,
        "precision": avg_precision,
        "recall": avg_recall,
        "total": total,
    }
    return json_obj



def dpo_results_unseen_common_macro_micro(
    model_responses
):
    dpo_accuracy_dict = {}
    dpo_p_dict = {}
    dpo_r_dict = {}
    dpo_test_numbers = {}
    skipped_criteria = 0
    tp = 0
    tpfp = 0
    tpfn = 0
    tptn = 0
    tpfptnfn = 0
    macacc = []
    macf1 = []
    for lab, lab_deatils in model_responses.items():
        criteria = list(lab_deatils.keys())
        dpo_lab_accuracy_dict = {}
        dpo_lab_p_dict = {}
        dpo_lab_r_dict = {}
        for criterion in criteria:
            original_grds = lab_deatils[criterion]["original_grades"]
            dpo_grds = lab_deatils[criterion]["predictions"]
            ratings = lab_deatils[criterion]["ratings"]
            llm_no_grades = len(dpo_grds)
            if llm_no_grades == 0:  # Skip if the JSON file was not found
                skipped_criteria += 1
                print(
                    f"Skipping criterion '{criterion}' due to missing JSON file for lab '{lab}'."
                )
                continue
            print(len(original_grds), len(dpo_grds), llm_no_grades, lab, criterion)
            dpo_ans = calculate_metrics(original_grds, dpo_grds)
            dpo_lab_accuracy_dict[criterion] = dpo_ans["accuracy"]
            dpo_lab_p_dict[criterion] = dpo_ans["precision"]
            dpo_lab_r_dict[criterion] = dpo_ans["recall"]
            temp_l = len(dpo_grds)
            tp += dpo_ans["micro_tp"]
            tpfp += dpo_ans["micro_tpfp"]
            tpfn += dpo_ans["micro_tpfn"]
            tptn += dpo_ans["micro_tptn"]
            tpfptnfn += dpo_ans["micro_total"]
            for _ in dpo_ans["macroacc"]:
                macacc.append(_)
            for _ in dpo_ans["macrof1"]:
                macf1.append(_)
            # print(dpo_grds, criterion, lab, 123123123123)
        dpo_accuracy_dict[lab] = dpo_lab_accuracy_dict
        dpo_p_dict[lab] = dpo_lab_p_dict
        dpo_r_dict[lab] = dpo_lab_r_dict
        dpo_test_numbers[lab] = temp_l
    dpo_macro = macro_accuracy(dpo_accuracy_dict)
    dpo_macro_overall = macro_overall_accuracy(dpo_macro)
    dpo_overall = micro_overall_accuracy(dpo_accuracy_dict, dpo_test_numbers)
    dpo_overallp = micro_overall_accuracy(dpo_p_dict, dpo_test_numbers)
    dpo_overallr = micro_overall_accuracy(dpo_r_dict, dpo_test_numbers)
    dpo_overallf = 2 * dpo_overallp * dpo_overallr / (dpo_overallp + dpo_overallr)
    # print()
    microp = tp / tpfp
    micror = tp / tpfn
    print(f"Micro overall Accuracy:{dpo_overall:.4f}")
    print(f"Precision:{dpo_overallp:.4f}")
    print(f"Recall:{dpo_overallr:.4f}")
    print(f"F1score:{dpo_overallf:.4f}")
    print(f"Macro overall Accuracy:{dpo_macro_overall:.4f}")
    # print(zs_macro)
    print()
    print(f"New Micro Acc:{tptn/tpfptnfn:.4f}")
    print(f"New Micro F1:{2*microp*micror/(micror+microp):.4f}")
    print(f"New Macro Acc:{sum(macacc)/len(macacc):.4f}")
    print(f"New Macro F1:{sum(macf1)/len(macf1):.4f}")
    # print(f"New Macro F1:{len(macacc)}", len(macf1))
    metrics = {}
    metrics["Micro overall Accuracy"] = dpo_overall
    metrics["Precision"] = dpo_overallp
    metrics["Recall"] = dpo_overallr
    metrics["F1score"] = dpo_overallf
    metrics["Macro overall Accuracy"] = dpo_macro_overall

    return metrics